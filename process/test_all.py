import sys
import textwrap
from collections import defaultdict
from pathlib import Path
from typing import Dict, Tuple, Optional
import pickle
import os
import numpy as np
import torch
import yaml
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from process.classfy import get_classfy

# --- é¡¹ç›®æ¨¡å— ---
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from process.datahandlers import get_handler
from process.embedders import get_embedder_by_name


# ========================================================================
# å…¨å±€é…ç½®
# ========================================================================
EMBEDDER_NAME = "prographer"
CLASSIFY_NAME = "prographer"

SEQUENCE_LENGTH_L = 12
DETECTION_THRESHOLD = 0.016
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# ========================================================================
# å·¥å…·å‡½æ•°
# ========================================================================
def save_snapshot_nodes(all_snapshots, output_file: Path = Path("snapshot.txt")) -> Optional[Path]:
    """ä¿å­˜å¿«ç…§èŠ‚ç‚¹ä¿¡æ¯åˆ°æ–‡ä»¶"""
    print(f"[INFO] ä¿å­˜å¿«ç…§èŠ‚ç‚¹ä¿¡æ¯åˆ°: {output_file}")
    try:
        with output_file.open("w", encoding="utf-8") as f:
            f.write("=== ProGrapher å¿«ç…§èŠ‚ç‚¹è¯¦æƒ…æŠ¥å‘Š ===\n")
            f.write(f"æ€»å¿«ç…§æ•°: {len(all_snapshots)}\n")
            f.write("=" * 60 + "\n\n")

            for i, snapshot in enumerate(all_snapshots):
                f.write(f"å¿«ç…§ {i}:\n")
                f.write(f"  èŠ‚ç‚¹æ€»æ•°: {len(snapshot.vs)}\n")
                f.write(f"  è¾¹æ€»æ•°: {len(snapshot.es)}\n")
                f.write("  èŠ‚ç‚¹è¯¦æƒ…:\n")

                node_type_count = defaultdict(int)
                malicious_count = sum(v["label"] == 1 for v in snapshot.vs)

                for v in snapshot.vs:
                    node_name = v["name"]
                    node_type = v.attributes().get("type_name", "UNKNOWN")
                    freq = v.attributes().get("frequency", "UNKNOWN")
                    label = v.attributes().get("label", 0)

                    node_type_count[node_type] += 1
                    status = "ğŸ”´æ¶æ„" if label == 1 else "ğŸŸ¢æ­£å¸¸"
                    f.write(f"    {node_name} | ç±»å‹:{node_type} | çŠ¶æ€:{status} ï½œ é¢‘ç‡:{freq}\n")

                f.write(f"  æ¶æ„èŠ‚ç‚¹æ•°: {malicious_count}/{len(snapshot.vs)}\n")
                f.write("  èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:\n")
                for t, c in sorted(node_type_count.items()):
                    f.write(f"      {t}: {c}ä¸ª\n")
                f.write("\n" + "-" * 50 + "\n\n")

        print(f"[INFO] å¿«ç…§ä¿¡æ¯å·²å†™å…¥ {output_file}")
        return output_file
    except Exception as e:
        print(f"[ERROR] ä¿å­˜å¿«ç…§å¤±è´¥: {e}")
        return None


def get_true_labels(snapshots) -> np.ndarray:
    """æå–å¿«ç…§çœŸå®æ ‡ç­¾"""
    return np.array([int(any(v["label"] == 1 for v in s.vs)) for s in snapshots])

def print_debug_info(all_snapshots, eval_true, eval_pred, eval_start_idx):
    """
    è¯¦ç»†æ‰“å°TPã€FPã€FNã€TNå¿«ç…§çš„è°ƒè¯•ä¿¡æ¯ï¼Œæ˜¾ç¤ºå¯¼è‡´åˆ†ç±»çš„å…·ä½“èŠ‚ç‚¹ã€‚
    """
    print("\n" + "="*70)
    print(" ğŸ” è¯¦ç»†è°ƒè¯•ä¿¡æ¯ (TP / FP / FN / TN å®Œæ•´åˆ†æ)")
    print("="*70)

    # åˆ†ç±»æ”¶é›†å„ç§æƒ…å†µçš„å¿«ç…§ç´¢å¼•
    tp_indices = []  # çœŸé˜³æ€§ï¼šçœŸå®æ¶æ„ + é¢„æµ‹æ¶æ„
    fp_indices = []  # å‡é˜³æ€§ï¼šçœŸå®è‰¯æ€§ + é¢„æµ‹æ¶æ„
    fn_indices = []  # å‡é˜´æ€§ï¼šçœŸå®æ¶æ„ + é¢„æµ‹è‰¯æ€§
    tn_indices = []  # çœŸé˜´æ€§ï¼šçœŸå®è‰¯æ€§ + é¢„æµ‹è‰¯æ€§

    for i in range(len(eval_true)):
        snapshot_idx = i + eval_start_idx
        true_label = eval_true[i]
        pred_label = eval_pred[i]

        if true_label == 1 and pred_label == 1:
            tp_indices.append(snapshot_idx)
        elif true_label == 0 and pred_label == 1:
            fp_indices.append(snapshot_idx)
        elif true_label == 1 and pred_label == 0:
            fn_indices.append(snapshot_idx)
        else:  # true_label == 0 and pred_label == 0
            tn_indices.append(snapshot_idx)

    # æ‰“å°ç»Ÿè®¡æ¦‚è§ˆ
    print(f"\nğŸ“Š å¿«ç…§åˆ†ç±»ç»Ÿè®¡:")
    print(f"  âœ… çœŸé˜³æ€§ (TP): {len(tp_indices)} ä¸ªå¿«ç…§")
    print(f"  âŒ å‡é˜³æ€§ (FP): {len(fp_indices)} ä¸ªå¿«ç…§")
    print(f"  âš ï¸  å‡é˜´æ€§ (FN): {len(fn_indices)} ä¸ªå¿«ç…§")
    print(f"  âœ“  çœŸé˜´æ€§ (TN): {len(tn_indices)} ä¸ªå¿«ç…§")

    # === è¯¦ç»†åˆ†æ TP å¿«ç…§ ===
    if tp_indices:
        print("\n" + "="*50)
        print("âœ… çœŸé˜³æ€§ (TP) å¿«ç…§è¯¦ç»†åˆ†æ - æ­£ç¡®æ£€æµ‹åˆ°çš„æ¶æ„å¿«ç…§")
        print("="*50)
        for snapshot_idx in tp_indices:
            snapshot = all_snapshots[snapshot_idx]
            print(f"\nğŸ¯ å¿«ç…§ {snapshot_idx}:")

            # åˆ†æèŠ‚ç‚¹ç±»å‹
            malicious_nodes = []
            benign_nodes = []
            node_types_count = {}

            for v in snapshot.vs:
                node_name = v['name']
                node_type = v.attributes().get('type_name', 'UNKNOWN')
                frequency = v.attributes().get('frequency', 'UNKNOWN')
                node_types_count[node_type] = node_types_count.get(node_type, 0) + 1

                if v.attributes().get('label') == 1:
                    malicious_nodes.append(f"{node_name}({node_type})ã€{frequency}ã€‘")
                else:
                    benign_nodes.append(f"{node_name}({node_type})")

            print(f"  ğŸ“ˆ æ€»èŠ‚ç‚¹æ•°: {len(snapshot.vs)}, è¾¹æ•°: {len(snapshot.es)}")
            print(f"  ğŸ”´ æ¶æ„èŠ‚ç‚¹ ({len(malicious_nodes)}ä¸ª):")
            if malicious_nodes:
                malicious_str = ', '.join(malicious_nodes[:10])  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                if len(malicious_nodes) > 10:
                    malicious_str += f" ... (+{len(malicious_nodes)-10}ä¸ªæ›´å¤š)"
                print(f"      {malicious_str}")

            print(f"  ğŸ“Š èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {dict(sorted(node_types_count.items()))}")

    # === è¯¦ç»†åˆ†æ FP å¿«ç…§ ===
    if fp_indices:
        print("\n" + "="*50)
        print("âŒ å‡é˜³æ€§ (FP) å¿«ç…§è¯¦ç»†åˆ†æ - è¯¯æŠ¥çš„è‰¯æ€§å¿«ç…§")
        print("="*50)
        for snapshot_idx in fp_indices:
            snapshot = all_snapshots[snapshot_idx]
            print(f"\nğŸš¨ å¿«ç…§ {snapshot_idx} (è¯¯æŠ¥):")

            # åˆ†æèŠ‚ç‚¹ç±»å‹åˆ†å¸ƒï¼Œå¯»æ‰¾è¯¯æŠ¥åŸå› 
            node_types_count = {}
            suspicious_patterns = []
            all_nodes = []

            for v in snapshot.vs:
                node_name = v['name']
                node_type = v.attributes().get('type_name', 'UNKNOWN')
                node_types_count[node_type] = node_types_count.get(node_type, 0) + 1
                frequency = v.attributes().get('frequency', 'UNKNOWN')

                all_nodes.append(f"{node_name}({node_type})ã€{frequency}ã€‘")

                # æ£€æŸ¥å¯èƒ½å¯¼è‡´è¯¯æŠ¥çš„æ¨¡å¼
                if 'SUBJECT_PROCESS' in node_type and any(word in node_name.lower()
                                                          for word in ['system', 'admin', 'service', 'daemon']):
                    suspicious_patterns.append(f"ç³»ç»Ÿè¿›ç¨‹: {node_name}")
                elif 'NETFLOW' in node_type:
                    suspicious_patterns.append(f"ç½‘ç»œæµ: {node_name}")

            print(f"  ğŸ“ˆ æ€»èŠ‚ç‚¹æ•°: {len(snapshot.vs)}, è¾¹æ•°: {len(snapshot.es)}")
            print(f"  ğŸ“Š èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {dict(sorted(node_types_count.items()))}")

            if suspicious_patterns:
                print(f"  âš¡ å¯èƒ½çš„è¯¯æŠ¥åŸå› :")
                for pattern in suspicious_patterns[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    print(f"      â€¢ {pattern}")

            # æ˜¾ç¤ºéƒ¨åˆ†èŠ‚ç‚¹åç§°ç”¨äºåˆ†æ
            print(f"  ğŸ“ éƒ¨åˆ†èŠ‚ç‚¹ (å‰10ä¸ª):")
            sample_nodes = ', '.join(all_nodes[:10])
            if len(all_nodes) > 10:
                sample_nodes += f" ... (+{len(all_nodes)-10}ä¸ªæ›´å¤š)"
            wrapped_nodes = textwrap.fill(sample_nodes, width=70, initial_indent='      ', subsequent_indent='      ')
            print(wrapped_nodes)

    # === è¯¦ç»†åˆ†æ FN å¿«ç…§ ===
    if fn_indices:
        print("\n" + "="*50)
        print("âš ï¸ å‡é˜´æ€§ (FN) å¿«ç…§è¯¦ç»†åˆ†æ - æ¼æ£€çš„æ¶æ„å¿«ç…§")
        print("="*50)
        for snapshot_idx in fn_indices:
            snapshot = all_snapshots[snapshot_idx]
            print(f"\nâš ï¸  å¿«ç…§ {snapshot_idx} (æ¼æ£€):")

            # åˆ†æä¸ºä»€ä¹ˆè¿™äº›æ¶æ„èŠ‚ç‚¹æ²¡è¢«æ£€æµ‹åˆ°
            malicious_nodes = []
            benign_nodes = []
            node_types_count = {}

            for v in snapshot.vs:
                node_name = v['name']
                node_type = v.attributes().get('type_name', 'UNKNOWN')
                node_types_count[node_type] = node_types_count.get(node_type, 0) + 1
                frequency = v.attributes().get('frequency', 'UNKNOWN')

                if v.attributes().get('label') == 1:
                    malicious_nodes.append(f"{node_name}({node_type})ã€{frequency}ã€‘")
                else:
                    benign_nodes.append(f"{node_name}({node_type})")

            print(f"  ğŸ“ˆ æ€»èŠ‚ç‚¹æ•°: {len(snapshot.vs)}, è¾¹æ•°: {len(snapshot.es)}")
            print(f"  ğŸ”´ è¢«æ¼æ£€çš„æ¶æ„èŠ‚ç‚¹ ({len(malicious_nodes)}ä¸ª):")
            if malicious_nodes:
                malicious_str = ', '.join(malicious_nodes)
                wrapped_malicious = textwrap.fill(malicious_str, width=70, initial_indent='      ', subsequent_indent='      ')
                print(wrapped_malicious)

            print(f"  ğŸ“Š èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {dict(sorted(node_types_count.items()))}")
            print(f"  ğŸ’¡ å¯èƒ½çš„æ¼æ£€åŸå› : æ¶æ„èŠ‚ç‚¹æ¯”ä¾‹è¾ƒä½ ({len(malicious_nodes)}/{len(snapshot.vs)} = {len(malicious_nodes)/len(snapshot.vs)*100:.1f}%)")

    # === ç®€è¦æ˜¾ç¤º TN å¿«ç…§ç»Ÿè®¡ ===
    if tn_indices:
        print("\n" + "="*50)
        print("âœ“ çœŸé˜´æ€§ (TN) å¿«ç…§ç»Ÿè®¡ - æ­£ç¡®è¯†åˆ«çš„è‰¯æ€§å¿«ç…§")
        print("="*50)
        print(f"  âœ… å…±æœ‰ {len(tn_indices)} ä¸ªå¿«ç…§è¢«æ­£ç¡®è¯†åˆ«ä¸ºè‰¯æ€§")

        # ç»Ÿè®¡TNå¿«ç…§çš„èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
        if len(tn_indices) > 0:
            sample_tn = all_snapshots[tn_indices[0]]  # å–ä¸€ä¸ªæ ·æœ¬
            tn_node_types = {}
            for v in sample_tn.vs:
                node_type = v.attributes().get('type_name', 'UNKNOWN')
                tn_node_types[node_type] = tn_node_types.get(node_type, 0) + 1
                frequency = v.attributes().get('frequency', 'UNKNOWN')

            print(f"  ğŸ“Š å…¸å‹è‰¯æ€§å¿«ç…§çš„èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ (å¿«ç…§{tn_indices[0]}): {dict(sorted(tn_node_types.items()))}ã€{frequency}ã€‘")

    print("\n" + "="*70)
    print("ğŸ¯ è°ƒè¯•åˆ†ææ€»ç»“:")
    print(f"  â€¢ æ€»å…±åˆ†æäº† {len(eval_true)} ä¸ªå¿«ç…§")
    print(f"  â€¢ æ£€æµ‹å‡†ç¡®ç‡: {(len(tp_indices) + len(tn_indices))/len(eval_true)*100:.1f}%")
    if len(tp_indices) + len(fn_indices) > 0:
        print(f"  â€¢ æ¶æ„å¿«ç…§å¬å›ç‡: {len(tp_indices)/(len(tp_indices) + len(fn_indices))*100:.1f}%")
    if len(tp_indices) + len(fp_indices) > 0:
        print(f"  â€¢ æ¶æ„æ£€æµ‹ç²¾ç¡®ç‡: {len(tp_indices)/(len(tp_indices) + len(fp_indices))*100:.1f}%")
    print("="*70)

def predict_snapshots(
    snapshot_embeddings: np.ndarray,
    model_path: Path,
) -> Tuple[np.ndarray, Dict]:
    """é¢„æµ‹å¿«ç…§å¼‚å¸¸æ ‡ç­¾"""

    classify = get_classfy(CLASSIFY_NAME)
    classify.load(model_path)
    pred_labels, diff_vectors  = classify.predict(snapshot_embeddings)

    return pred_labels, diff_vectors


def run_evaluation(detector_model_path: Path, encoder_model_path: Path, path_map: dict) -> None:
    snapshot_file = "snapshot_data.pkl"
    if not os.path.exists(snapshot_file):
        print(f"âŒ é”™è¯¯ï¼šå¿«ç…§æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {snapshot_file}")
        print("è¯·å…ˆè¿è¡Œ train_darpa.py æ¥ç”Ÿæˆå¿«ç…§æ•°æ®")
        return

    with open(snapshot_file, 'rb') as f:
        snapshot_data = pickle.load(f)

    # æå–å¿«ç…§æ•°æ®
    all_snapshots = snapshot_data['all_snapshots']
    benign_idx_start = snapshot_data['benign_idx_start']
    benign_idx_end = snapshot_data['benign_idx_end']
    malicious_idx_start = snapshot_data['malicious_idx_start']
    malicious_idx_end = snapshot_data['malicious_idx_end']

    print(f"âœ… å¿«ç…§æ•°æ®åŠ è½½æˆåŠŸ:")
    print(f"  - æ€»å¿«ç…§æ•°: {len(all_snapshots)}")
    print(f"  - è‰¯æ€§å¿«ç…§èŒƒå›´: {benign_idx_start} åˆ° {benign_idx_end}")
    print(f"  - æ¶æ„å¿«ç…§èŒƒå›´: {malicious_idx_start} åˆ° {malicious_idx_end}")
    mal_snapshots = all_snapshots[malicious_idx_start: malicious_idx_end + 1]
    if not mal_snapshots:
        print("[ERROR] æœªèƒ½æ„å»ºå¿«ç…§")
        return

    # ä¿å­˜å¿«ç…§èŠ‚ç‚¹ä¿¡æ¯åˆ°æ–‡ä»¶
    save_snapshot_nodes(mal_snapshots)
    true_labels = get_true_labels(mal_snapshots)

    print("\n[DEBUG] å¿«ç…§ä¿¡æ¯")
    print(f"  - æ€»å¿«ç…§æ•°: {len(mal_snapshots)}")
    print(f"  - çœŸå®æ ‡ç­¾æ•°: {len(true_labels)}")
    print(f"  - çœŸå®æ ‡ç­¾: {true_labels.tolist()}")

    embedder_cls = get_embedder_by_name(EMBEDDER_NAME)
    embedder = embedder_cls.load(encoder_model_path, snapshot_sequence=all_snapshots)
    snapshot_embeddings = embedder.get_snapshot_embeddings()

    pred_labels, diff_vectors = predict_snapshots(
        snapshot_embeddings[malicious_idx_start: malicious_idx_end + 1],
        detector_model_path,
    )
    print(f"æ£€æµ‹åˆ° {len(diff_vectors)} ä¸ªå¼‚å¸¸å¿«ç…§")
    print(f"é¢„æµ‹æ ‡ç­¾é•¿åº¦: {len(pred_labels)}")

    acc = accuracy_score(true_labels, pred_labels)
    prec = precision_score(true_labels, pred_labels, zero_division=0)
    rec = recall_score(true_labels, pred_labels, zero_division=0)
    f1 = f1_score(true_labels, pred_labels, zero_division=0)
    tp = np.sum((true_labels == 1) & (pred_labels == 1))
    fp = np.sum((true_labels == 0) & (pred_labels == 1))
    tn = np.sum((true_labels == 0) & (pred_labels == 0))
    fn = np.sum((true_labels == 1) & (pred_labels == 0))

    print("\n=== è¯„ä¼°ç»“æœ ===")
    print("\n" + "=" * 50)
    print(" å¿«ç…§çº§åˆ«è¯„ä¼°ç»“æœ (æ‰€æœ‰å¿«ç…§)")
    print("=" * 50)
    print(f" çœŸé˜³æ€§ (TP): {tp}")
    print(f" å‡é˜³æ€§ (FP): {fp}")
    print(f" çœŸé˜´æ€§ (TN): {tn}")
    print(f" å‡é˜´æ€§ (FN): {fn}")
    print("\n æ€§èƒ½è¯„åˆ†:")
    print(f" å‡†ç¡®ç‡: {acc:.4f}")
    print(f" ç²¾ç¡®ç‡: {prec:.4f}")
    print(f" å¬å›ç‡: {rec:.4f}")
    print(f" F1åˆ†æ•°: {f1:.4f}")
    print("=" * 50)
    print_debug_info(mal_snapshots, true_labels, pred_labels, 0)  # ä»ç´¢å¼•0å¼€å§‹



# ========================================================================
# ä¸»å…¥å£
# ========================================================================
if __name__ == "__main__":
    with open("config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    env = config["local"] if "windows" in sys.platform else config["remote"]
    detector_path, encoder_path = Path(env["DETECTOR_MODEL_PATH"]), Path(env["ENCODER_MODEL_PATH"])

    if not detector_path.exists():
        sys.exit(f"[ERROR] æ£€æµ‹å™¨æ¨¡å‹ä¸å­˜åœ¨: {detector_path}")
    if not encoder_path.exists():
        sys.exit(f"[ERROR] ç¼–ç å™¨æ¨¡å‹ä¸å­˜åœ¨: {encoder_path}")

    run_evaluation(detector_path, encoder_path, env["path_map"])