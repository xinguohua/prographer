import os
import sys
import yaml
import textwrap  # å¯¼å…¥ textwrap ä»¥ä¾¿æ ¼å¼åŒ–é•¿æ–‡æœ¬
from collections import defaultdict
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import platform
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm

# --- 1. è®¾ç½®å’Œå¯¼å…¥ ---
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from process.datahandlers import get_handler
from process.embedders import ProGrapherEmbedder

# --- 2. æ¨¡å‹å®šä¹‰ (ä¸å˜) ---
class AnomalyDetector(nn.Module):
    # ... æ­¤å¤„ä»£ç æ— å˜åŒ– ...
    def __init__(self, embedding_dim, hidden_dim, num_layers, dropout, kernel_sizes, num_filters):
        super(AnomalyDetector, self).__init__()
        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout if num_layers > 1 else 0)
        self.convs = nn.ModuleList([nn.Conv1d(in_channels=hidden_dim * 2, out_channels=num_filters, kernel_size=k) for k in kernel_sizes])
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(num_filters * len(kernel_sizes), embedding_dim)
    def forward(self, sequence_embeddings):
        lstm_out, _ = self.lstm(sequence_embeddings)
        conv_in = lstm_out.permute(0, 2, 1)
        pooled_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(conv_in))
            pooled = F.max_pool1d(conv_out, kernel_size=conv_out.shape[2]).squeeze(2)
            pooled_outputs.append(pooled)
        concatenated = torch.cat(pooled_outputs, dim=1)
        dropped_out = self.dropout(concatenated)
        predicted_embedding = self.fc(dropped_out)
        return predicted_embedding

# --- 3. è¶…å‚æ•°è®¾ç½® (å¯é€šè¿‡å‚æ•°è‡ªå®šä¹‰) ---
SEQUENCE_LENGTH_L = 12
EMBEDDING_DIM = 256
HIDDEN_DIM = 128
NUM_LAYERS = 5
DROPOUT_RATE = 0.2
KERNEL_SIZES = [3, 4, 5]
NUM_FILTERS = 100
DETECTION_THRESHOLD = 0.016
TOP_K_INDICATORS = 5
WL_DEPTH = 4
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# --- 4. è¾…åŠ©å‡½æ•° (ä¸å˜) ---
def save_snapshot_nodes_to_file(all_snapshots, output_dir="d:/prographer/process"):
    """å°†æ¯ä¸ªå¿«ç…§ä¸­çš„èŠ‚ç‚¹ä¿¡æ¯ä¿å­˜åˆ°txtæ–‡ä»¶"""
    import os
    from datetime import datetime
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(output_dir, f"snapshot_nodes_{timestamp}.txt")
    
    print(f"\n--- ä¿å­˜å¿«ç…§èŠ‚ç‚¹ä¿¡æ¯åˆ°æ–‡ä»¶ ---")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=== ProGrapher å¿«ç…§èŠ‚ç‚¹è¯¦æƒ…æŠ¥å‘Š ===\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»å¿«ç…§æ•°: {len(all_snapshots)}\n")
            f.write("=" * 60 + "\n\n")
            
            for i, snapshot in enumerate(all_snapshots):
                f.write(f"å¿«ç…§ {i}:\n")
                f.write(f"  èŠ‚ç‚¹æ€»æ•°: {len(snapshot.vs)}\n")
                f.write(f"  è¾¹æ€»æ•°: {len(snapshot.es)}\n")
                f.write("  èŠ‚ç‚¹è¯¦æƒ…:\n")
                
                # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹
                node_type_count = {}
                malicious_count = 0
                
                for v in snapshot.vs:
                    node_name = v['name']
                    node_type = v.attributes().get('type_name', 'UNKNOWN')
                    label = v.attributes().get('label', 0)
                    
                    # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹
                    node_type_count[node_type] = node_type_count.get(node_type, 0) + 1
                    
                    # ç»Ÿè®¡æ¶æ„èŠ‚ç‚¹
                    if label == 1:
                        malicious_count += 1
                    
                    # å†™å…¥èŠ‚ç‚¹ä¿¡æ¯
                    status = "ğŸ”´æ¶æ„" if label == 1 else "ğŸŸ¢æ­£å¸¸"
                    f.write(f"    {node_name} | ç±»å‹:{node_type} | çŠ¶æ€:{status}\n")
                
                # å†™å…¥ç»Ÿè®¡ä¿¡æ¯
                f.write(f"  ç»Ÿè®¡ä¿¡æ¯:\n")
                f.write(f"    æ¶æ„èŠ‚ç‚¹æ•°: {malicious_count}/{len(snapshot.vs)}\n")
                f.write(f"    èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:\n")
                for node_type, count in sorted(node_type_count.items()):
                    f.write(f"      {node_type}: {count}ä¸ª\n")
                f.write("\n" + "-" * 50 + "\n\n")
        
        print(f"âœ… å¿«ç…§èŠ‚ç‚¹ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_file}")
        return output_file
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¿«ç…§èŠ‚ç‚¹ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        return None

def predict_anomalous_snapshots(snapshot_embeddings, model_path, dynamic_sequence_length=None):
    # ã€ä¿®æ”¹ã€‘åŠ¨æ€è°ƒæ•´å·ç§¯æ ¸å¤§å°ä»¥é€‚åº”åºåˆ—é•¿åº¦
    effective_sequence_length = dynamic_sequence_length if dynamic_sequence_length is not None else SEQUENCE_LENGTH_L
    
    # ã€æ–°å¢ã€‘æ ¹æ®åºåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´å·ç§¯æ ¸ï¼Œç¡®ä¿ä¸ä¼šå‡ºé”™
    max_kernel_size = max(KERNEL_SIZES)
    if effective_sequence_length < max_kernel_size:
        # å¦‚æœåºåˆ—é•¿åº¦å°äºæœ€å¤§å·ç§¯æ ¸ï¼Œè°ƒæ•´å·ç§¯æ ¸å¤§å°
        adjusted_kernel_sizes = [k for k in KERNEL_SIZES if k <= effective_sequence_length]
        if not adjusted_kernel_sizes:
            adjusted_kernel_sizes = [1]  # è‡³å°‘ä¿æŒä¸€ä¸ªå·ç§¯æ ¸
        print(f"è­¦å‘Š: åºåˆ—é•¿åº¦({effective_sequence_length})å°äºæœ€å¤§å·ç§¯æ ¸({max_kernel_size})")
        print(f"è°ƒæ•´å·ç§¯æ ¸ä» {KERNEL_SIZES} åˆ° {adjusted_kernel_sizes}")
        kernel_sizes_to_use = adjusted_kernel_sizes
    else:
        kernel_sizes_to_use = KERNEL_SIZES
    
    detector_model = AnomalyDetector(
        embedding_dim=EMBEDDING_DIM, 
        hidden_dim=HIDDEN_DIM, 
        num_layers=NUM_LAYERS, 
        dropout=DROPOUT_RATE, 
        kernel_sizes=kernel_sizes_to_use,  # ä½¿ç”¨è°ƒæ•´åçš„å·ç§¯æ ¸
        num_filters=NUM_FILTERS
    ).to(device)
    
    detector_model.load_state_dict(torch.load(model_path, map_location=device))
    detector_model.eval()
    tensor = torch.tensor(snapshot_embeddings, dtype=torch.float32)
    
    # ã€ä¿®æ”¹ã€‘ä¸ºæ‰€æœ‰å¿«ç…§åˆå§‹åŒ–é¢„æµ‹æ ‡ç­¾
    snapshot_pred_labels = np.zeros(len(tensor), dtype=int)
    diff_vectors = {}
    snapshot_scores = {}  # ã€æ–°å¢ã€‘å­˜å‚¨æ¯ä¸ªå¿«ç…§çš„å¾—åˆ†
    
    print(f"å¿«ç…§åµŒå…¥å¼ é‡å½¢çŠ¶: {tensor.shape}")
    print(f"é¢„æµ‹æ ‡ç­¾æ•°ç»„åˆå§‹é•¿åº¦: {len(snapshot_pred_labels)}")
    print(f"ä½¿ç”¨åºåˆ—é•¿åº¦: {effective_sequence_length}")
    print(f"ä½¿ç”¨å·ç§¯æ ¸å¤§å°: {kernel_sizes_to_use}")
    
    total_snapshots = len(tensor)
    
    with torch.no_grad():
        # ã€å…³é”®ä¿®æ”¹ã€‘ä¸ºäº†æ£€æµ‹æ‰€æœ‰å¿«ç…§ï¼ˆåŒ…æ‹¬ç¬¬ä¸€ä¸ªï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸åŒçš„ç­–ç•¥
        for i in tqdm(range(total_snapshots), desc="æ£€æµ‹å¿«ç…§åºåˆ—", leave=True, unit="snapshot"):
            
            if i < effective_sequence_length:
                # ã€æ–°ç­–ç•¥ã€‘å¯¹äºå‰é¢çš„å¿«ç…§ï¼Œä½¿ç”¨é›¶å¡«å……æˆ–é‡å¤å¡«å……
                if i == 0:
                    # ç¬¬ä¸€ä¸ªå¿«ç…§ï¼šä½¿ç”¨å½“å‰å¿«ç…§é‡å¤å¡«å……
                    sequence = tensor[0].unsqueeze(0).repeat(effective_sequence_length, 1)
                else:
                    # å‰é¢å‡ ä¸ªå¿«ç…§ï¼šä½¿ç”¨é›¶å¡«å…… + å¯ç”¨å¿«ç…§
                    padding_size = effective_sequence_length - i
                    padding = torch.zeros(padding_size, tensor.shape[1])
                    available_snapshots = tensor[:i]
                    sequence = torch.cat([padding, available_snapshots], dim=0)
                
                target = tensor[i]
            else:
                # ã€æ ‡å‡†ç­–ç•¥ã€‘ä½¿ç”¨æ»‘åŠ¨çª—å£
                sequence = tensor[i-effective_sequence_length:i]
                target = tensor[i]
            
            # é¢„æµ‹å’Œæ£€æµ‹
            sequence = sequence.unsqueeze(0).to(device)
            target = target.unsqueeze(0).to(device)
            prediction = detector_model(sequence).squeeze(0)
            error = torch.nn.functional.mse_loss(prediction, target).item()
            diff_vector = (prediction - target).cpu().numpy()
            
            # ã€æ–°å¢ã€‘è®°å½•æ¯ä¸ªå¿«ç…§çš„å¾—åˆ†
            snapshot_scores[i] = error
            
            # æ ‡è®°å¼‚å¸¸
            if error > DETECTION_THRESHOLD:
                snapshot_pred_labels[i] = 1
                diff_vectors[i] = {
                    "position": i, 
                    "error": error, 
                    "diff_vector": diff_vector, 
                    "real_embedding": target.cpu().numpy(), 
                    "pred_embedding": prediction.cpu().numpy()
                }
    
    # ã€æ–°å¢ã€‘æ‰“å°æ¯ä¸ªå¿«ç…§çš„å¾—åˆ†
    print(f"\n--- å¿«ç…§å¾—åˆ†è¯¦æƒ… ---")
    print(f"æ£€æµ‹é˜ˆå€¼: {DETECTION_THRESHOLD}")
    print("å¿«ç…§ç´¢å¼• | å¾—åˆ†(Error) | çŠ¶æ€")
    print("-" * 40)
    for i in range(total_snapshots):
        score = snapshot_scores[i]
        status = "ğŸ”´ å¼‚å¸¸" if score > DETECTION_THRESHOLD else "ğŸŸ¢ æ­£å¸¸"
        print(f"å¿«ç…§ {i:2d}   | {score:.6f}   | {status}")
    print("-" * 40)
    
    return snapshot_pred_labels, diff_vectors

def get_true_snapshot_labels(snapshots):
    # ... æ­¤å‡½æ•°å†…å®¹ä¸å˜ ...
    true_labels = []
    for snapshot in snapshots:
        malicious_nodes = any(v['label'] == 1 for v in snapshot.vs)
        true_labels.append(1 if malicious_nodes else 0)
    return np.array(true_labels)

def generate_key_indicators(all_snapshots, diff_vectors, rsg_embeddings, rsg_vocab):
    # ... æ­¤å‡½æ•°å†…å®¹ä¸å˜ ...
    print("\n" + "="*50); print(" å…³é”®æŒ‡æ ‡ç”Ÿæˆå™¨ - å¼‚å¸¸RSGæ’å"); print("="*50)
    if not diff_vectors: print("æœªæ£€æµ‹åˆ°ä»»ä½•å¼‚å¸¸å¿«ç…§ï¼Œè·³è¿‡æŒ‡æ ‡ç”Ÿæˆ"); return
    progress = tqdm(diff_vectors.items(), total=len(diff_vectors), desc="åˆ†æå¼‚å¸¸å¿«ç…§")
    for idx, anomaly_info in progress:
        # ã€ä¿®å¤ã€‘æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨å¿«ç…§èŒƒå›´å†…
        if idx >= len(all_snapshots):
            print(f"\nè­¦å‘Š: å¼‚å¸¸å¿«ç…§ç´¢å¼• {idx} è¶…å‡ºå¿«ç…§èŒƒå›´ (0-{len(all_snapshots)-1})ï¼Œè·³è¿‡")
            continue
            
        snapshot = all_snapshots[idx]; diff_vector = anomaly_info["diff_vector"]
        if diff_vector.ndim > 1: diff_vector = diff_vector.squeeze()
        rsg_scores = defaultdict(float); rsg_count = 0
        for v_idx in range(len(snapshot.vs)):
            for d in range(WL_DEPTH + 1):
                rsg_str = ProGrapherEmbedder.generate_rsg(snapshot, v_idx, d)
                if rsg_str in rsg_vocab:
                    rsg_id = rsg_vocab[rsg_str]; rsg_vec = rsg_embeddings[rsg_id]
                    if rsg_vec.ndim > 1: rsg_vec = rsg_vec.squeeze()
                    score = np.abs(np.vdot(diff_vector, rsg_vec)); rsg_scores[rsg_str] = max(rsg_scores[rsg_str], score); rsg_count += 1
        if not rsg_scores: progress.set_postfix(snapshot=f"{idx}", info=f"æœªæ‰¾åˆ°RSG"); continue
        sorted_rsgs = sorted(rsg_scores.items(), key=lambda x: x[1], reverse=True)
        progress.set_postfix(snapshot=f"{idx}", score=f"{sorted_rsgs[0][1]:.4f}")
        print(f"\nå¼‚å¸¸å¿«ç…§ {idx} (æ£€æµ‹å·®å¼‚: {anomaly_info['error']:.6f})"); print(f"  - æ€»RSGæ•°é‡: {rsg_count}"); print(f"  - Top-{TOP_K_INDICATORS} å¯ç–‘ RSG:")
        for i, (rsg, score) in enumerate(sorted_rsgs[:TOP_K_INDICATORS]): print(f"    {i+1}. {rsg} (å¯ç–‘åº¦: {score:.6f})")
    print("="*50 + "\n")

# =========================================================================
# =================== å¢å¼ºçš„è°ƒè¯•ä¿¡æ¯å‡½æ•° ==================================
# =========================================================================
def print_debug_info(all_snapshots, eval_true, eval_pred, eval_start_idx):
    """
    è¯¦ç»†æ‰“å°TPã€FPã€FNã€TNå¿«ç…§çš„è°ƒè¯•ä¿¡æ¯ï¼Œæ˜¾ç¤ºå¯¼è‡´åˆ†ç±»çš„å…·ä½“èŠ‚ç‚¹ã€‚
    """
    print("\n" + "="*70)
    print(" ğŸ” è¯¦ç»†è°ƒè¯•ä¿¡æ¯ (TP / FP / FN / TN å®Œæ•´åˆ†æ)")
    print("="*70)

    # åˆ†ç±»æ”¶é›†å„ç§æƒ…å†µçš„å¿«ç…§ç´¢å¼•
    tp_indices = []  # çœŸé˜³æ€§ï¼šçœŸå®æ¶æ„ + é¢„æµ‹æ¶æ„
    fp_indices = []  # å‡é˜³æ€§ï¼šçœŸå®è‰¯æ€§ + é¢„æµ‹æ¶æ„
    fn_indices = []  # å‡é˜´æ€§ï¼šçœŸå®æ¶æ„ + é¢„æµ‹è‰¯æ€§
    tn_indices = []  # çœŸé˜´æ€§ï¼šçœŸå®è‰¯æ€§ + é¢„æµ‹è‰¯æ€§

    for i in range(len(eval_true)):
        snapshot_idx = i + eval_start_idx
        true_label = eval_true[i]
        pred_label = eval_pred[i]

        if true_label == 1 and pred_label == 1:
            tp_indices.append(snapshot_idx)
        elif true_label == 0 and pred_label == 1:
            fp_indices.append(snapshot_idx)
        elif true_label == 1 and pred_label == 0:
            fn_indices.append(snapshot_idx)
        else:  # true_label == 0 and pred_label == 0
            tn_indices.append(snapshot_idx)

    # æ‰“å°ç»Ÿè®¡æ¦‚è§ˆ
    print(f"\nğŸ“Š å¿«ç…§åˆ†ç±»ç»Ÿè®¡:")
    print(f"  âœ… çœŸé˜³æ€§ (TP): {len(tp_indices)} ä¸ªå¿«ç…§")
    print(f"  âŒ å‡é˜³æ€§ (FP): {len(fp_indices)} ä¸ªå¿«ç…§")
    print(f"  âš ï¸  å‡é˜´æ€§ (FN): {len(fn_indices)} ä¸ªå¿«ç…§")
    print(f"  âœ“  çœŸé˜´æ€§ (TN): {len(tn_indices)} ä¸ªå¿«ç…§")

    # === è¯¦ç»†åˆ†æ TP å¿«ç…§ ===
    if tp_indices:
        print("\n" + "="*50)
        print("âœ… çœŸé˜³æ€§ (TP) å¿«ç…§è¯¦ç»†åˆ†æ - æ­£ç¡®æ£€æµ‹åˆ°çš„æ¶æ„å¿«ç…§")
        print("="*50)
        for snapshot_idx in tp_indices:
            snapshot = all_snapshots[snapshot_idx]
            print(f"\nğŸ¯ å¿«ç…§ {snapshot_idx}:")

            # åˆ†æèŠ‚ç‚¹ç±»å‹
            malicious_nodes = []
            benign_nodes = []
            node_types_count = {}

            for v in snapshot.vs:
                node_name = v['name']
                node_type = v.attributes().get('type_name', 'UNKNOWN')
                node_types_count[node_type] = node_types_count.get(node_type, 0) + 1

                if v.attributes().get('label') == 1:
                    malicious_nodes.append(f"{node_name}({node_type})")
                else:
                    benign_nodes.append(f"{node_name}({node_type})")

            print(f"  ğŸ“ˆ æ€»èŠ‚ç‚¹æ•°: {len(snapshot.vs)}, è¾¹æ•°: {len(snapshot.es)}")
            print(f"  ğŸ”´ æ¶æ„èŠ‚ç‚¹ ({len(malicious_nodes)}ä¸ª):")
            if malicious_nodes:
                malicious_str = ', '.join(malicious_nodes[:10])  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                if len(malicious_nodes) > 10:
                    malicious_str += f" ... (+{len(malicious_nodes)-10}ä¸ªæ›´å¤š)"
                print(f"      {malicious_str}")

            print(f"  ğŸ“Š èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {dict(sorted(node_types_count.items()))}")

    # === è¯¦ç»†åˆ†æ FP å¿«ç…§ ===
    if fp_indices:
        print("\n" + "="*50)
        print("âŒ å‡é˜³æ€§ (FP) å¿«ç…§è¯¦ç»†åˆ†æ - è¯¯æŠ¥çš„è‰¯æ€§å¿«ç…§")
        print("="*50)
        for snapshot_idx in fp_indices:
            snapshot = all_snapshots[snapshot_idx]
            print(f"\nğŸš¨ å¿«ç…§ {snapshot_idx} (è¯¯æŠ¥):")

            # åˆ†æèŠ‚ç‚¹ç±»å‹åˆ†å¸ƒï¼Œå¯»æ‰¾è¯¯æŠ¥åŸå› 
            node_types_count = {}
            suspicious_patterns = []
            all_nodes = []

            for v in snapshot.vs:
                node_name = v['name']
                node_type = v.attributes().get('type_name', 'UNKNOWN')
                node_types_count[node_type] = node_types_count.get(node_type, 0) + 1
                all_nodes.append(f"{node_name}({node_type})")

                # æ£€æŸ¥å¯èƒ½å¯¼è‡´è¯¯æŠ¥çš„æ¨¡å¼
                if 'SUBJECT_PROCESS' in node_type and any(word in node_name.lower()
                                                          for word in ['system', 'admin', 'service', 'daemon']):
                    suspicious_patterns.append(f"ç³»ç»Ÿè¿›ç¨‹: {node_name}")
                elif 'NETFLOW' in node_type:
                    suspicious_patterns.append(f"ç½‘ç»œæµ: {node_name}")

            print(f"  ğŸ“ˆ æ€»èŠ‚ç‚¹æ•°: {len(snapshot.vs)}, è¾¹æ•°: {len(snapshot.es)}")
            print(f"  ğŸ“Š èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {dict(sorted(node_types_count.items()))}")

            if suspicious_patterns:
                print(f"  âš¡ å¯èƒ½çš„è¯¯æŠ¥åŸå› :")
                for pattern in suspicious_patterns[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    print(f"      â€¢ {pattern}")

            # æ˜¾ç¤ºéƒ¨åˆ†èŠ‚ç‚¹åç§°ç”¨äºåˆ†æ
            print(f"  ğŸ“ éƒ¨åˆ†èŠ‚ç‚¹ (å‰10ä¸ª):")
            sample_nodes = ', '.join(all_nodes[:10])
            if len(all_nodes) > 10:
                sample_nodes += f" ... (+{len(all_nodes)-10}ä¸ªæ›´å¤š)"
            wrapped_nodes = textwrap.fill(sample_nodes, width=70, initial_indent='      ', subsequent_indent='      ')
            print(wrapped_nodes)

    # === è¯¦ç»†åˆ†æ FN å¿«ç…§ ===
    if fn_indices:
        print("\n" + "="*50)
        print("âš ï¸ å‡é˜´æ€§ (FN) å¿«ç…§è¯¦ç»†åˆ†æ - æ¼æ£€çš„æ¶æ„å¿«ç…§")
        print("="*50)
        for snapshot_idx in fn_indices:
            snapshot = all_snapshots[snapshot_idx]
            print(f"\nâš ï¸  å¿«ç…§ {snapshot_idx} (æ¼æ£€):")

            # åˆ†æä¸ºä»€ä¹ˆè¿™äº›æ¶æ„èŠ‚ç‚¹æ²¡è¢«æ£€æµ‹åˆ°
            malicious_nodes = []
            benign_nodes = []
            node_types_count = {}

            for v in snapshot.vs:
                node_name = v['name']
                node_type = v.attributes().get('type_name', 'UNKNOWN')
                node_types_count[node_type] = node_types_count.get(node_type, 0) + 1

                if v.attributes().get('label') == 1:
                    malicious_nodes.append(f"{node_name}({node_type})")
                else:
                    benign_nodes.append(f"{node_name}({node_type})")

            print(f"  ğŸ“ˆ æ€»èŠ‚ç‚¹æ•°: {len(snapshot.vs)}, è¾¹æ•°: {len(snapshot.es)}")
            print(f"  ğŸ”´ è¢«æ¼æ£€çš„æ¶æ„èŠ‚ç‚¹ ({len(malicious_nodes)}ä¸ª):")
            if malicious_nodes:
                malicious_str = ', '.join(malicious_nodes)
                wrapped_malicious = textwrap.fill(malicious_str, width=70, initial_indent='      ', subsequent_indent='      ')
                print(wrapped_malicious)

            print(f"  ğŸ“Š èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {dict(sorted(node_types_count.items()))}")
            print(f"  ğŸ’¡ å¯èƒ½çš„æ¼æ£€åŸå› : æ¶æ„èŠ‚ç‚¹æ¯”ä¾‹è¾ƒä½ ({len(malicious_nodes)}/{len(snapshot.vs)} = {len(malicious_nodes)/len(snapshot.vs)*100:.1f}%)")

    # === ç®€è¦æ˜¾ç¤º TN å¿«ç…§ç»Ÿè®¡ ===
    if tn_indices:
        print("\n" + "="*50)
        print("âœ“ çœŸé˜´æ€§ (TN) å¿«ç…§ç»Ÿè®¡ - æ­£ç¡®è¯†åˆ«çš„è‰¯æ€§å¿«ç…§")
        print("="*50)
        print(f"  âœ… å…±æœ‰ {len(tn_indices)} ä¸ªå¿«ç…§è¢«æ­£ç¡®è¯†åˆ«ä¸ºè‰¯æ€§")

        # ç»Ÿè®¡TNå¿«ç…§çš„èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
        if len(tn_indices) > 0:
            sample_tn = all_snapshots[tn_indices[0]]  # å–ä¸€ä¸ªæ ·æœ¬
            tn_node_types = {}
            for v in sample_tn.vs:
                node_type = v.attributes().get('type_name', 'UNKNOWN')
                tn_node_types[node_type] = tn_node_types.get(node_type, 0) + 1
            print(f"  ğŸ“Š å…¸å‹è‰¯æ€§å¿«ç…§çš„èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ (å¿«ç…§{tn_indices[0]}): {dict(sorted(tn_node_types.items()))}")

    print("\n" + "="*70)
    print("ğŸ¯ è°ƒè¯•åˆ†ææ€»ç»“:")
    print(f"  â€¢ æ€»å…±åˆ†æäº† {len(eval_true)} ä¸ªå¿«ç…§")
    print(f"  â€¢ æ£€æµ‹å‡†ç¡®ç‡: {(len(tp_indices) + len(tn_indices))/len(eval_true)*100:.1f}%")
    if len(tp_indices) + len(fn_indices) > 0:
        print(f"  â€¢ æ¶æ„å¿«ç…§å¬å›ç‡: {len(tp_indices)/(len(tp_indices) + len(fn_indices))*100:.1f}%")
    if len(tp_indices) + len(fp_indices) > 0:
        print(f"  â€¢ æ¶æ„æ£€æµ‹ç²¾ç¡®ç‡: {len(tp_indices)/(len(tp_indices) + len(fp_indices))*100:.1f}%")
    print("="*70)

# =========================================================================
# =================== æ ¸å¿ƒè¯„ä¼°å‡½æ•° (ä¿æŒä¸å˜) ===========================
# =========================================================================
def run_snapshot_level_evaluation(detector_model_path, encoder_model_path, PATH_MAP, MALICIOUS_INTERVALS_PATH,
                                  sequence_length=12,
                                  test_window_minutes=20,
                                  ):
    """è¿è¡Œå¿«ç…§çº§åˆ«çš„å¼‚å¸¸æ£€æµ‹è¯„ä¼° - è¯„ä¼°æ‰€æœ‰å¿«ç…§"""
    handler = get_handler(
        "atlas", 
        False,
        PATH_MAP,
        MALICIOUS_INTERVALS_PATH,
        use_time_split=True,
        test_window_minutes=test_window_minutes
    )
    handler.load()
    all_snapshots,complete_nodes_per_graph, labels_per_graph = handler.build_graph()
    if not all_snapshots: 
        print("é”™è¯¯: æœªèƒ½æ„å»ºä»»ä½•å¿«ç…§ã€‚")
        return
        
    # ä¿å­˜å¿«ç…§èŠ‚ç‚¹ä¿¡æ¯åˆ°æ–‡ä»¶
    save_snapshot_nodes_to_file(all_snapshots)
    
    true_labels = get_true_snapshot_labels(all_snapshots)
    
    print(f"\n--- è°ƒè¯•ä¿¡æ¯ ---")
    print(f"æ€»å¿«ç…§æ•°: {len(all_snapshots)}")
    print(f"çœŸå®æ ‡ç­¾æ•°: {len(true_labels)}")
    print(f"çœŸå®æ ‡ç­¾å†…å®¹: {true_labels}")

    print(f"âœ… å°†è¯„ä¼°æ‰€æœ‰ {len(all_snapshots)} ä¸ªå¿«ç…§")
    
    print("\n--- åŠ è½½é¢„è®­ç»ƒçš„ç¼–ç å™¨ ---")
    embedder = ProGrapherEmbedder.load(encoder_model_path, snapshot_sequence=all_snapshots)
    snapshot_embeddings = embedder.get_snapshot_embeddings()
    rsg_embeddings, rsg_vocab = embedder.get_rsg_embeddings()
    print(f"RSGåµŒå…¥åŠ è½½å®Œæ¯•ï¼Œè¯æ±‡å¤§å°: {len(rsg_vocab)}")
    
    # ã€å…³é”®ä¿®æ”¹ã€‘æ— è®ºå¿«ç…§æ•°é‡å¤šå°‘ï¼Œéƒ½è¿›è¡Œå¼‚å¸¸æ£€æµ‹
    pred_labels, diff_vectors = predict_anomalous_snapshots(
        snapshot_embeddings, detector_model_path
    )
    print(f"æ£€æµ‹åˆ° {len(diff_vectors)} ä¸ªå¼‚å¸¸å¿«ç…§")
    print(f"é¢„æµ‹æ ‡ç­¾é•¿åº¦: {len(pred_labels)}")
    
    # ã€å…³é”®ä¿®æ”¹ã€‘è¯„ä¼°æ‰€æœ‰å¿«ç…§ï¼Œä»ç´¢å¼•0å¼€å§‹
    eval_true = true_labels
    eval_pred = pred_labels
    
    print(f"âœ… è¯„ä¼°æ‰€æœ‰å¿«ç…§: 0 åˆ° {len(all_snapshots)-1}")
    print(f"è¯„ä¼°çœŸå®æ ‡ç­¾é•¿åº¦: {len(eval_true)}")
    print(f"è¯„ä¼°é¢„æµ‹æ ‡ç­¾é•¿åº¦: {len(eval_pred)}")
    
    # ç¡®ä¿ä¸¤ä¸ªæ•°ç»„é•¿åº¦ä¸€è‡´
    min_len = min(len(eval_true), len(eval_pred))
    if min_len == 0:
        print("é”™è¯¯: æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œè¯„ä¼°")
        return
    
    eval_true = eval_true[:min_len]
    eval_pred = eval_pred[:min_len]
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    tp = np.sum((eval_true == 1) & (eval_pred == 1))
    fp = np.sum((eval_true == 0) & (eval_pred == 1))
    tn = np.sum((eval_true == 0) & (eval_pred == 0))
    fn = np.sum((eval_true == 1) & (eval_pred == 0))
    
    acc = accuracy_score(eval_true, eval_pred)
    prec = precision_score(eval_true, eval_pred, zero_division=0)
    rec = recall_score(eval_true, eval_pred, zero_division=0)
    f1 = f1_score(eval_true, eval_pred, zero_division=0)
    
    print("\n" + "="*50)
    print(" å¿«ç…§çº§åˆ«è¯„ä¼°ç»“æœ (æ‰€æœ‰å¿«ç…§)")
    print("="*50)
    print(f" çœŸé˜³æ€§ (TP): {tp}")
    print(f" å‡é˜³æ€§ (FP): {fp}")
    print(f" çœŸé˜´æ€§ (TN): {tn}")
    print(f" å‡é˜´æ€§ (FN): {fn}")
    print("\n æ€§èƒ½è¯„åˆ†:")
    print(f" å‡†ç¡®ç‡: {acc:.4f}")
    print(f" ç²¾ç¡®ç‡: {prec:.4f}")
    print(f" å¬å›ç‡: {rec:.4f}")
    print(f" F1åˆ†æ•°: {f1:.4f}")
    print("="*50)
    
    generate_key_indicators(all_snapshots, diff_vectors, rsg_embeddings, rsg_vocab)
    print_debug_info(all_snapshots, eval_true, eval_pred, 0)  # ä»ç´¢å¼•0å¼€å§‹

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == '__main__':

    with open("config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    system = platform.system().lower()
    if "windows" in system:
        env_config = config["local"]
    else:
        env_config = config["remote"]



    # æ‹¿åˆ°è·¯å¾„
    DETECTOR_MODEL_PATH = env_config["DETECTOR_MODEL_PATH"]
    ENCODER_MODEL_PATH = env_config["ENCODER_MODEL_PATH"]
    MALICIOUS_INTERVALS_PATH = env_config["malicious_intervals"]
    PATH_MAP = env_config["path_map"]

    # ã€æ–°å¢ã€‘å¯è‡ªå®šä¹‰çš„å‚æ•°
    SEQUENCE_LENGTH = 7        # åºåˆ—é•¿åº¦ï¼Œå¯ä»¥ä¿®æ”¹
    TEST_WINDOW = 20           # æµ‹è¯•çª—å£åˆ†é’Ÿæ•°ï¼Œå¯ä»¥ä¿®æ”¹
    
    print(f"ä½¿ç”¨å‚æ•°: åºåˆ—é•¿åº¦={SEQUENCE_LENGTH},  æµ‹è¯•çª—å£={TEST_WINDOW}åˆ†é’Ÿ")
    
    if not os.path.exists(DETECTOR_MODEL_PATH): print(f"é”™è¯¯: æ£€æµ‹å™¨æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {DETECTOR_MODEL_PATH}"); sys.exit(1)
    if not os.path.exists(ENCODER_MODEL_PATH): print(f"é”™è¯¯: ç¼–ç å™¨æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {ENCODER_MODEL_PATH}\næç¤º: è¯·å…ˆè¿è¡Œ train.py æ¥è®­ç»ƒå¹¶ç”Ÿæˆç¼–ç å™¨æ¨¡å‹ã€‚"); sys.exit(1)
    
    run_snapshot_level_evaluation(DETECTOR_MODEL_PATH, ENCODER_MODEL_PATH, PATH_MAP, MALICIOUS_INTERVALS_PATH,
                                 SEQUENCE_LENGTH,  TEST_WINDOW)